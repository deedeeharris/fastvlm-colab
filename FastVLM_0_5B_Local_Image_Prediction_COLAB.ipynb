{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_R7Z2BVlG2"
      },
      "source": [
        "#  üì∑ FastVLM-0.5B: Efficient Vision-Language Model Image Captioning **Demo**\n",
        "\n",
        "[Yedidya Harris](https://www.linkedin.com/in/yedidya-harris/), Last Updated: 04/09/2025"
      ],
      "id": "L-_R7Z2BVlG2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the **Apple FastVLM-0.5B Vision-Language Model** Colab notebook! This notebook lets you run state-of-the-art image captioning and visual question answering using **FastVLM-0.5B**, a new AI model from Apple engineered for extreme speed and high accuracy‚Äîeven on compact devices.\n",
        "\n",
        "- **FastVLM-0.5B** is up to 85x faster than previous models like LLaVA-OneVision and is 3.4x smaller, making it perfect for on-device intelligence on iPhones, iPads, and Macs.\n",
        "- The model efficiently processes images and text together for tasks such as **image description**, **question answering**, and general image analysis, all with minimal latency.\n",
        "- The notebook provides a simple interface to upload an image, input your prompt, and instantly generate captions or answers using the model‚Äôs latest architecture.\n",
        "\n",
        "> **Tip:** Here in Google Colab, you can enable GPU acceleration for much faster processing by selecting `Runtime > Change runtime type > Hardware accelerator > GPU` from the menu. If you prefer, you can simply run on CPU. Both options are supported. Choosing GPU can significantly decrease latency.\n",
        "\n",
        "### What You Can Do in This Notebook\n",
        "\n",
        "- Upload any image to analyze or describe\n",
        "- Enter a custom prompt to guide the model (e.g., \"How many objects are visible?\" or \"Describe this scene.\")\n",
        "- See results in real time with fast end-to-end processing\n",
        "\n",
        "This demo showcases the ease-of-use, efficiency, and powerful capabilities of Apple‚Äôs latest vision-language models‚Äîfeaturing **on-device privacy**, low resource requirements, and robust accuracy in practical AI scenarios.\n"
      ],
      "metadata": {
        "id": "n23WkOqRwSj8"
      },
      "id": "n23WkOqRwSj8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "4GDK_NPQfHlh"
      },
      "id": "4GDK_NPQfHlh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation\n",
        "!pip install git+https://github.com/huggingface/transformers.git \\\n",
        "            git+https://github.com/huggingface/accelerate.git \\\n",
        "            pillow torch einops bitsandbytes -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwqIBujFd4bQ",
        "outputId": "78274dda-e3f6-4322-c2ec-f876a34f061b"
      },
      "id": "qwqIBujFd4bQ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Define the model ID for apple/FastVLM-0.5B\n",
        "MODEL_ID = \"apple/FastVLM-0.5B\"\n",
        "# Define the special token index for the image placeholder\n",
        "IMAGE_TOKEN_INDEX = -200\n",
        "\n",
        "# Check for GPU availability and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the tokenizer from the pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "# Load the model with appropriate settings for the available device\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    # Use float16 for GPU acceleration, otherwise float32 for CPU\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ").eval()\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "c4ab45553ab24717b98b930843a6804c",
            "c49c66bc00964e42bb022e86de44001a",
            "668ede716e2145c9af6ab6d8795256eb",
            "3046372d268542f39235c07b7dc33e82",
            "2b6cdde879b64e048daee9ed04dc7e97",
            "b1ddb970a9004839893ec87eac51f089",
            "01f1edc57d724e9e84757f2bdad54b36",
            "64c7a95387fb48469ed59239d40b30cf",
            "2b76e586cede4796ac4b4def37c3b3eb",
            "60dabf5202714d6ca8a6a2f1ea846023",
            "087ec95168fd4fa0b73ecbce7ed84b2b",
            "565a17e796dc469ebfde50fce1fb20ae",
            "9fb37b4ecd5c475eaa669f75afec0749",
            "bf579e12e42549f49f67b7f3fed6f1f3",
            "5451052d96f84077a96eb22df48f8f7b",
            "ecf893e1a86341478ba4c875cadb4072",
            "d93ebf85c75b46dba3a5be7213295188",
            "f2a5dc0cbe1743c68c800a2c966e5e1d",
            "8a31cda43e0944c2a18f5122bce78ebe",
            "b702de9bbcb04aaeac3dc344fea0d576",
            "6c72de8cf2494536beb2100f6734086f",
            "893b1048ce5845b2929b83fc8cbd4789",
            "16665e7b801a4638a386da9fbee5ad03",
            "98d9b313d7204120892f79e21eb37b88",
            "f2a8892b204441e5833152649a1d8e19",
            "7ea20e2d0ae8471eaa707ee7dc5cd674",
            "9899f9c594a54796bff82acc16a1b112",
            "a4163f54bf2040828df46bb4a4ad02d1",
            "4e2a40fe860145dc94d300ec3aa92857",
            "7f57d1729dca4adfaa5318e21548a0cb",
            "dd7c8c30ab9e46adbbd1049317685dd0",
            "0f0255c82dd7484082cc1f56941c52ea",
            "e06efb1cfdf64ba1bba940bba9ec53bc",
            "6b89bfaa222b4c39b0224f9155fdd120",
            "d9106719675c43a09a182fd28ae163d1",
            "f42109533c984abe934ca02e758c7360",
            "20f30cc30adb48d7b4e961a2d6713ed9",
            "c3952d22c51f4212aef391cd6b9c01b4",
            "77ec66ea06de445d8dd06de47efb6332",
            "5101ab2c076b4aa690233c4c3d8c0afb",
            "fe47d0e365fa4be2bfcf4f5855437ee1",
            "f8cf91d38e324b109382768849a418b2",
            "9d308c90742141c9a398d1eb7ba89dcf",
            "77634ef10bdb43228c8d6469de42f34e",
            "3aa186fe2709414c9b0db896f328a970",
            "77916c3c0e8c4ce98f8edefdba5f0722",
            "03c79d75e6ed4d64ae9f40c31c837015",
            "2c6aed993c1241e2a63ce25523fdb284",
            "855d8527929b4ba396625009bf000711",
            "f960cfd22fd2438b802c85b75422776f",
            "feb7a82213bd4c2188b93bd9265e3a28",
            "66e89a5bd24446c8b5ffb5af2034cf4e",
            "b7d2593375614d929ed2a2ac07e9859b",
            "0e3dcb05767a408699c7ad5788487f56",
            "6bd08700b60f4b0b9b2e84e1682e5b69",
            "ea605dd5498c49508bd0b4b6dd088b14",
            "62df1e90c1b449a487577cfac9157eae",
            "28715c66ebc34395938e5658f1aff547",
            "7a5913cb091145fa99676cbef4dc4efc",
            "2ca10ab56c2c462d8b7649871270169e",
            "e340e85c1ede4c1d86ed5b938fc081a6",
            "c29dd15603d84ab5bb868504803a6c37",
            "fd51d398aa77434abc5354abcf289f52",
            "5c4ff1e0659945129c0df4c8ca791623",
            "b753550d07c045da9efaa4f42f2c702a",
            "e47153af51974758961ee75cb971c4c0",
            "8cafa41759dd4a668333fa19ac187840",
            "8a774b978d284201aa9e505361646a75",
            "5c487362e1f94f388506f979b000363c",
            "43c18ed357524bbba21cb4c8380fe546",
            "d44ccdd41f4d463a92e5d2ab770717fe",
            "4cc04a4ed0804a50adecc39fe4b4d2b3",
            "8b3c90b8edb44eadbf1ed0347628d7f1",
            "bdce36306ef64a9d81bfac2c6639dbd3",
            "b22c22ff66074e33a0b2eae6225b5174",
            "ed8a0d5e437240e1a7832326949d8c8f",
            "c968f78f0c8e4ec9a191ca5dfde1fc11",
            "a11e5e850ff7494496d4574ca23439f6",
            "2f8ce9eeaae74cdcbca947fe0cac73b9",
            "f4e5ec68c8f84cd28ff82bef7aaa6ab4",
            "d3cf9535a91d4db99886e1cf0e207417",
            "cc7015d446f34741b015786c7b8992c8",
            "d0773b315fb84066b077dc32dd5c04e0",
            "d74c243c7d964489b447bb6a6d7eb4e1",
            "c1d422177c4c43ff8186e6ec8749dc6f",
            "79a13b4f66f248a784d38bf5473732b3",
            "f76dd85af8c9489f912e1d9e84f13ce5",
            "00989f92ab6641fb8bec1a641a834f71",
            "dfd8c7933b5744fd9ab981957eaa0679",
            "33ee01e026ff4b1db407a2e7b7a58159",
            "dd5cda93655349b9beb55521149a428a",
            "acf3667ada3345eabf01168ad3bb5292",
            "b31cf277d4e44a9bbf0f4f1f0ec7b8bf",
            "10fcc7b48d0e4c7ea9d0503a37285380",
            "02786a1cf4494dfebcd87b5a9a656890",
            "d66cabe27246425fbda7a8a7a358a683",
            "2eecf4d373ff4aed8d5d0a8603b379c5",
            "390de08a6f504601a835b785c421f590",
            "22f4458bde2247b38175d2117a26031a"
          ]
        },
        "id": "ku5hhLgAdyOR",
        "outputId": "3fd8bd35-5d64-4e61-9b3e-87b870636584"
      },
      "id": "ku5hhLgAdyOR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4ab45553ab24717b98b930843a6804c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "565a17e796dc469ebfde50fce1fb20ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16665e7b801a4638a386da9fbee5ad03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b89bfaa222b4c39b0224f9155fdd120"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3aa186fe2709414c9b0db896f328a970"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea605dd5498c49508bd0b4b6dd088b14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llava_qwen.py: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cafa41759dd4a668333fa19ac187840"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/apple/FastVLM-0.5B:\n",
            "- llava_qwen.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a11e5e850ff7494496d4574ca23439f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/100 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfd8c7933b5744fd9ab981957eaa0679"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# accepts both an Image object and an image file path (string).\n",
        "def predict_from_image(\n",
        "    image,  # Removed type hint to allow for both string and Image object\n",
        "    prompt_input: str,\n",
        "    max_new_tokens: int = 128,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.9,\n",
        "    top_k: int = 50,\n",
        "    repetition_penalty: float = 1.1\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a caption for an image, accepting either a file path or a Pillow Image object.\n",
        "\n",
        "    Args:\n",
        "        image: The path to the local image file (str) or a Pillow Image object.\n",
        "        prompt_input (str): The text prompt to guide the model's generation.\n",
        "        max_new_tokens (int, optional): Max tokens to generate. Defaults to 128.\n",
        "        temperature (float, optional): Sampling temperature. Defaults to 0.7.\n",
        "        top_p (float, optional): Nucleus sampling probability. Defaults to 0.9.\n",
        "        top_k (int, optional): Top-k sampling. Defaults to 50.\n",
        "        repetition_penalty (float, optional): Penalty for repeating tokens. Defaults to 1.1.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated caption for the image.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the input is a file path (string) and load the image\n",
        "        if isinstance(image, str):\n",
        "            image = Image.open(image)\n",
        "\n",
        "        # Ensure the image is in RGB format\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "        # Step 2: Prepare the inputs for the model\n",
        "        # Create the chat message structure with the image placeholder\n",
        "        messages = [{\"role\": \"user\", \"content\": f\"<image>\\n{prompt_input}\"}]\n",
        "        rendered = tokenizer.apply_chat_template(\n",
        "            messages, add_generation_prompt=True, tokenize=False\n",
        "        )\n",
        "\n",
        "        # Split the text around the placeholder, tokenize, and reassemble with the image token\n",
        "        pre, post = rendered.split(\"<image>\", 1)\n",
        "        pre_ids  = tokenizer(pre,  return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
        "        post_ids = tokenizer(post, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
        "        img_tok = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype)\n",
        "        input_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(model.device)\n",
        "        attention_mask = torch.ones_like(input_ids, device=model.device)\n",
        "\n",
        "        # Process the image using the model's vision tower to get pixel values\n",
        "        pixel_values = model.get_vision_tower().image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "        pixel_values = pixel_values.to(model.device, dtype=model.dtype)\n",
        "\n",
        "        # Step 3: Generate the caption using the model\n",
        "        with torch.no_grad():\n",
        "            out = model.generate(\n",
        "                inputs=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                images=pixel_values,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                top_k=top_k,\n",
        "                repetition_penalty=repetition_penalty,\n",
        "                do_sample=True if temperature > 0 else False,\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens into a string\n",
        "        response = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "        # Clean the response to only return the assistant's part\n",
        "        cleaned_response = response.split(\"assistant\")[-1].strip()\n",
        "        return cleaned_response\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: The image file was not found at '{image}'.\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {e}\"\n"
      ],
      "metadata": {
        "id": "uyWp3X_yd-SX"
      },
      "id": "uyWp3X_yd-SX",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "qjAeZ0UrfT5T"
      },
      "id": "qjAeZ0UrfT5T"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run me\n",
        "import time\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import os\n",
        "\n",
        "# Create a dedicated output widget to display status messages and the loading indicator\n",
        "status_output = widgets.Output()\n",
        "\n",
        "def on_run_button_clicked(b):\n",
        "    \"\"\"Handles the first button click to trigger the file upload.\"\"\"\n",
        "    with status_output:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Please upload an image...\")\n",
        "\n",
        "    # Trigger the file upload. The rest of the logic will be handled\n",
        "    # by a second, independent function after the file is uploaded.\n",
        "    uploaded_files = files.upload()\n",
        "    process_uploaded_file(uploaded_files)\n",
        "\n",
        "def process_uploaded_file(uploaded_files):\n",
        "    \"\"\"Handles the file once it's uploaded and runs the prediction.\"\"\"\n",
        "    if not uploaded_files:\n",
        "        with status_output:\n",
        "            clear_output(wait=True)\n",
        "            print(\"No file was uploaded. Please try again.\")\n",
        "        return\n",
        "\n",
        "    # Get the name of the uploaded file\n",
        "    uploaded_filename = list(uploaded_files.keys())[0]\n",
        "\n",
        "    try:\n",
        "        # Open the uploaded image\n",
        "        image = Image.open(uploaded_filename)\n",
        "\n",
        "        with status_output:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Image '{uploaded_filename}' uploaded successfully.\")\n",
        "\n",
        "            # Display the interactive loading indicator before prediction starts\n",
        "            display(HTML(\"\"\"\n",
        "                <div style=\"display: flex; align-items: center; padding-top: 10px;\">\n",
        "                    <i class=\"fa fa-spinner fa-spin fa-2x\" style=\"color: #4285F4;\"></i>\n",
        "                    <p style=\"margin-left: 10px; font-size: 25px;\">Running model prediction...</p>\n",
        "                </div>\n",
        "            \"\"\"))\n",
        "\n",
        "        # Get the prompt from the text box\n",
        "        prompt = prompt_input.value\n",
        "\n",
        "        # --- timing here ---\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Call the prediction function\n",
        "        caption = predict_from_image(image, prompt)\n",
        "\n",
        "        # Calculate the elapsed time\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        # Clear the loading indicator and print the final results\n",
        "        with status_output:\n",
        "            clear_output(wait=True)\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"Uploaded Image: {uploaded_filename}\")\n",
        "            print(f\"Prompt: {prompt}\")\n",
        "            print(f\"Generated Caption: {caption}\")\n",
        "            print(f\"Time taken: {elapsed_time:.4f} seconds\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "    except Exception as e:\n",
        "        with status_output:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up the uploaded file to avoid clutter\n",
        "        if os.path.exists(uploaded_filename):\n",
        "            os.remove(uploaded_filename)\n",
        "\n",
        "# --- UI widgets ---\n",
        "\n",
        "# Text input for the prompt\n",
        "prompt_input = widgets.Text(\n",
        "    value='What does this image show?',\n",
        "    placeholder='Enter your prompt here',\n",
        "    description='Prompt:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Run button\n",
        "run_button = widgets.Button(\n",
        "    description='Upload Image & Run',\n",
        "    button_style='success',\n",
        "    tooltip='Click to upload an image and run the prediction.'\n",
        ")\n",
        "\n",
        "# Link the button click event to the first function\n",
        "run_button.on_click(on_run_button_clicked)\n",
        "\n",
        "# Display the widgets in a VBox container\n",
        "display(widgets.VBox([\n",
        "    widgets.VBox([\n",
        "        widgets.HTML(\"<b>Step 1:</b> Enter your prompt in the box below.\"),\n",
        "        prompt_input\n",
        "    ]),\n",
        "    widgets.VBox([\n",
        "        widgets.HTML(\"<b>Step 2:</b> Click the button to select and upload your image.\"),\n",
        "        run_button\n",
        "    ]),\n",
        "    status_output\n",
        "]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-IlYnFq-d7cl"
      },
      "id": "-IlYnFq-d7cl",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4GDK_NPQfHlh"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
	
	
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
