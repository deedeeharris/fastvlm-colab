{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting from a Local Image Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the same packages used for the webcam demo.\n",
    "# Running this cell is optional if you have already run the installation cell at the top of the notebook.\n",
    "!pip install git+https://github.com/huggingface/transformers.git \\\n",
    "             git+https://github.com/huggingface/accelerate.git \\\n",
    "             pillow torch einops bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the model ID for apple/FastVLM-0.5B\n",
    "MODEL_ID = \"apple/FastVLM-0.5B\"\n",
    "# Define the special token index for the image placeholder\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "\n",
    "# Check for GPU availability and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer from the pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# Load the model with appropriate settings for the available device\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    # Use float16 for GPU acceleration, otherwise float32 for CPU\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def predict_from_image_path(\n",
    "    image_path: str,\n",
    "    prompt_input: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    repetition_penalty: float = 1.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a caption for an image located at a local file path.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the local image file.\n",
    "        prompt_input (str): The text prompt to guide the model's generation.\n",
    "        max_new_tokens (int, optional): Max tokens to generate. Defaults to 128.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to 0.7.\n",
    "        top_p (float, optional): Nucleus sampling probability. Defaults to 0.9.\n",
    "        top_k (int, optional): Top-k sampling. Defaults to 50.\n",
    "        repetition_penalty (float, optional): Penalty for repeating tokens. Defaults to 1.1.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated caption for the image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load the image from the specified path and convert to RGB\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Step 2: Prepare the inputs for the model\n",
    "        # Create the chat message structure with the image placeholder\n",
    "        messages = [{\"role\": \"user\", \"content\": f\"<image>\\n{prompt_input}\"}]\n",
    "        rendered = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, tokenize=False\n",
    "        )\n",
    "\n",
    "        # Split the text around the placeholder, tokenize, and reassemble with the image token\n",
    "        pre, post = rendered.split(\"<image>\", 1)\n",
    "        pre_ids  = tokenizer(pre,  return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "        post_ids = tokenizer(post, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "        img_tok = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype)\n",
    "        input_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(model.device)\n",
    "        attention_mask = torch.ones_like(input_ids, device=model.device)\n",
    "\n",
    "        # Process the image using the model's vision tower to get pixel values\n",
    "        pixel_values = model.get_vision_tower().image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        pixel_values = pixel_values.to(model.device, dtype=model.dtype)\n",
    "\n",
    "        # Step 3: Generate the caption using the model\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                inputs=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=pixel_values,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                do_sample=True if temperature > 0 else False,\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into a string\n",
    "        response = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        # Clean the response to only return the assistant's part\n",
    "        cleaned_response = response.split(\"assistant\")[-1].strip()\n",
    "        return cleaned_response\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: The image file was not found at '{image_path}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# --- Create a dummy image for demonstration ---\n",
    "# This step is just to make the example self-contained and runnable.\n",
    "# In a real scenario, you would replace 'dummy_image.png' with your own image path.\n",
    "try:\n",
    "    # Create a blank blue image\n",
    "    img = Image.new('RGB', (400, 300), color = (73, 109, 137))\n",
    "    d = ImageDraw.Draw(img)\n",
    "\n",
    "    # Add some text to the image\n",
    "    try:\n",
    "        # Try to load a common font\n",
    "        font = ImageFont.truetype(\"LiberationSans-Regular.ttf\", 20)\n",
    "    except IOError:\n",
    "        # If the font is not available, use a basic default one\n",
    "        font = ImageFont.load_default()\n",
    "    d.text((50,130), \"This is a test image.\", fill=(255,255,0), font=font)\n",
    "\n",
    "    # Define the path and save the image\n",
    "    dummy_image_path = \"dummy_image.png\"\n",
    "    img.save(dummy_image_path)\n",
    "    print(f\"Created a dummy image at: {dummy_image_path}\")\n",
    "\n",
    "    # --- Example usage of the prediction function ---\n",
    "    # Define the path to your local image\n",
    "    image_path = dummy_image_path\n",
    "\n",
    "    # Define the prompt for the model\n",
    "    prompt = \"What does this image show? Describe the color and text.\"\n",
    "\n",
    "    # Call the function to get the caption\n",
    "    caption = predict_from_image_path(image_path, prompt)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Image Path: {image_path}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated Caption: {caption}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the example usage: {e}\")\n",
    "    print(\"This might happen if font files are not available on the system.\")\n",
    "    print(\"The prediction function itself should still work with your own images.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FastVLM_Local_Image_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
